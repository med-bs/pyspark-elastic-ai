{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9551b6e-b899-471c-b6ef-8cf2d75538e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n",
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-8.7.0-py3-none-any.whl (387 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.9/387.9 kB\u001b[0m \u001b[31m828.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting elastic-transport<9,>=8\n",
      "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m112.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from elastic-transport<9,>=8->elasticsearch) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<2,>=1.26.2 in /opt/conda/lib/python3.10/site-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.15)\n",
      "Installing collected packages: elastic-transport, elasticsearch\n",
      "Successfully installed elastic-transport-8.4.0 elasticsearch-8.7.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "\n",
    "!pip install elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a38135-4cfd-4fc7-946d-0441ca71a3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.elasticsearch:elasticsearch-spark-30_2.12:7.16.2,com.fasterxml.jackson.module:jackson-module-scala_2.12:2.13.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473cf6a-b786-4972-bf5a-b212e194b2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName(\"kafka_elastic_test2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbaf5ab-b77f-4c10-81e9-f3e93198f5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a Kafka stream for transaction\n",
    "df_saving_transaction_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account_transaction\") \\\n",
    "    .load()\n",
    "\n",
    "# create a Kafka stream for account\n",
    "df_saving_account_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account\") \\\n",
    "    .load()\n",
    "\n",
    "# create a Kafka stream for transaction\n",
    "df_current_transaction_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account_transaction\") \\\n",
    "    .load()\n",
    "\n",
    "# create a Kafka stream for account\n",
    "df_current_account_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39b5628-d9d9-469f-990a-727211c205c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType, TimestampType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_transaction = StructType([\n",
    "    StructField(\"account-id\", LongType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"customer-id\", LongType(), True),\n",
    "    StructField(\"datetime\", StringType(), True),\n",
    "    StructField(\"is_fraud\", StringType(), True),\n",
    "    StructField(\"transaction-id\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_account = StructType([\n",
    "    StructField(\"customer-id\",  LongType(), True),\n",
    "    StructField(\"account-id\", LongType(), True)\n",
    "])\n",
    "\n",
    "account_df = spark.createDataFrame([], schema=schema_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46231f19-f26b-456d-9ed5-6bb974e33a41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31318/4050940603.py:28: ElasticsearchWarning: Elasticsearch built-in security features are not enabled. Without authentication, your cluster could be accessible to anyone. See https://www.elastic.co/guide/en/elasticsearch/reference/7.17/security-minimal-setup.html to enable security.\n",
      "  if not es.indices.exists(index=es_index):\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import RequestError\n",
    "\n",
    "# Define the Elasticsearch index name\n",
    "es_index = \"my_index_test\"\n",
    "es_port = 9200\n",
    "es_host = \"elasticsearch\"\n",
    "\n",
    "# Create an Elasticsearch client\n",
    "es = Elasticsearch([{\"host\": es_host, \"port\": es_port, \"scheme\": \"http\"}])\n",
    "\n",
    "# Create a mapping for the Elasticsearch index\n",
    "# Define the index mapping\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"account-id\": {\"type\": \"long\"},\n",
    "            \"amount\": {\"type\": \"double\"},\n",
    "            \"customer-id\": {\"type\": \"long\"},\n",
    "            \"datetime\": {\"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss\"},\n",
    "            \"is_fraud\": {\"type\": \"keyword\"},\n",
    "            \"transaction-id\": {\"type\": \"long\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if the index exists, and create it if it does not\n",
    "if not es.indices.exists(index=es_index):\n",
    "    try:\n",
    "        # Create the Elasticsearch index if it doesn't exist\n",
    "        es.indices.create(index=es_index, body=mapping)\n",
    "    except RequestError as e:\n",
    "        print(f\"Index creation failed: {e}\")\n",
    "        exit(1)\n",
    "    print(f\"Created Elasticsearch index '{es_index}'\")\n",
    "    \n",
    "# Write the streaming DataFrame to Elasticsearch\n",
    "def write_to_es(es_df):\n",
    "    es_df.show()\n",
    "    es_df.write \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", es_host) \\\n",
    "    .option(\"es.port\", es_port) \\\n",
    "    .option(\"es.resource\", es_index) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03e5523-4c73-4511-ad71-d2839806fafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, date_format\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import base64\n",
    "import decimal\n",
    "from time import sleep\n",
    "\n",
    "def getDecimalFromKafka(encoded):\n",
    "    \n",
    "    # Decode the Base64 encoded string and create a BigInteger from it\n",
    "    decoded = decimal.Decimal(int.from_bytes(base64.b64decode(encoded), byteorder='big', signed=False))\n",
    "\n",
    "    # Create a context object with the specified scale\n",
    "    context = decimal.Context(prec=28, rounding=decimal.ROUND_HALF_DOWN)\n",
    "\n",
    "    # Set the scale of the decimal value using the context object\n",
    "    decimal_value = decoded.quantize(decimal.Decimal('.1') ** 3, context=context)\n",
    "\n",
    "    return decimal_value/1000000\n",
    "\n",
    "def write_to_es_transaction(df_t, epoch_id):\n",
    "    \n",
    "    row_transactions = df_t.collect()\n",
    "    \n",
    "    for row_transaction in row_transactions:\n",
    "    \n",
    "      # if(row_transaction):\n",
    "            value_dict_transaction = json.loads(row_transaction.value)\n",
    "        \n",
    "            timestamp = value_dict_transaction['payload']['after']['created_date']/1000\n",
    "            # convert Unix timestamp to a datetime object\n",
    "            dt = datetime.datetime.fromtimestamp(timestamp)\n",
    "            # format datetime object as \"yyyy-mm-dd hh:mm:ss\"\n",
    "            formatted_dt = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "            account_id = value_dict_transaction['payload']['after']['savings_account_id']\n",
    "        \n",
    "            while account_df.filter(col(\"account-id\") == account_id).count() == 0:\n",
    "                # Wait for 0.1 second before checking again\n",
    "                sleep(0.1)\n",
    "        \n",
    "            # Code to execute after the condition becomes true\n",
    "            # Filter the DataFrame to get rows where \"account-id\" is in account_df[\"account-id\"]\n",
    "            filtered_account_df = account_df.filter(account_df[\"account-id\"] == account_id)\n",
    "            # Select the \"customer-id\" column from the filtered DataFrame\n",
    "            cutomer_id = filtered_account_df.select(\"customer-id\").collect()[0][0]\n",
    "        \n",
    "            new_row_transaction = spark.createDataFrame([(account_id,\n",
    "                                                      float(getDecimalFromKafka(value_dict_transaction['payload']['after']['amount'])),\n",
    "                                                      cutomer_id,\n",
    "                                                      formatted_dt,\n",
    "                                                     # date_format(from_unixtime(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "                                                      'valid',#-----test after will be predected by ML\n",
    "                                                      value_dict_transaction['payload']['after']['id'],\n",
    "                                                     )], schema=schema_transaction)\n",
    "        \n",
    "            write_to_es(new_row_transaction)\n",
    "        \n",
    "        \n",
    "def write_to_es_account(df_a, epoch_id):\n",
    "    \n",
    "    global account_df\n",
    "        \n",
    "    row_accounts = df_a.collect()\n",
    "    \n",
    "    for row_account in row_accounts :\n",
    "    \n",
    "       #if(row_account):\n",
    "            value_dict_account = json.loads(row_account.value)\n",
    "            new_row_account= spark.createDataFrame([(value_dict_account['payload']['after']['client_id'],\n",
    "                                                      value_dict_account['payload']['after']['id'],\n",
    "                                                     )], schema=schema_account)\n",
    "        \n",
    "            # Check if new_row_account is already present in acount_df\n",
    "            if account_df.subtract(new_row_account).count() == account_df.count():\n",
    "                # new_row_account does not exist in acount_df, so concatenate the two DataFrames\n",
    "                account_df = account_df.union(new_row_account)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa07dd-f2a7-4be5-a36a-747472e32f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+-------------------+--------+--------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|\n",
      "+----------+------+-----------+-------------------+--------+--------------+\n",
      "|         2| 300.1|          2|2023-05-08 12:26:34|   valid|            45|\n",
      "+----------+------+-----------+-------------------+--------+--------------+\n",
      "\n",
      "+----------+------+-----------+-------------------+--------+--------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|\n",
      "+----------+------+-----------+-------------------+--------+--------------+\n",
      "|         2|1255.0|          2|2023-05-08 11:56:36|   valid|            44|\n",
      "+----------+------+-----------+-------------------+--------+--------------+\n",
      "\n",
      "+----------+------+-----------+-------------------+--------+--------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|\n",
      "+----------+------+-----------+-------------------+--------+--------------+\n",
      "|         2| 300.1|          2|2023-05-08 12:26:34|   valid|            45|\n",
      "+----------+------+-----------+-------------------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the write_to_es function on each micro-batch of data\n",
    "value_df_account = df_account_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_account = value_df_account.writeStream.foreachBatch(write_to_es_account).start()\n",
    "\n",
    "value_df_transaction = df_transaction_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_transaction = value_df_transaction.writeStream.foreachBatch(write_to_es_transaction).start()\n",
    "\n",
    "# Wait for the stream to finish\n",
    "#query_account.awaitTermination()\n",
    "query_transaction.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
