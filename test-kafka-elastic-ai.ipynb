{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9551b6e-b899-471c-b6ef-8cf2d75538e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a38135-4cfd-4fc7-946d-0441ca71a3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4473cf6a-b786-4972-bf5a-b212e194b2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName(\"kafkaConsumer\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbaf5ab-b77f-4c10-81e9-f3e93198f5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a Kafka stream for transaction\n",
    "df_transaction_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account_transaction\") \\\n",
    "    .load()\n",
    "\n",
    "# create a Kafka stream for account\n",
    "df_account_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95eb74e1-6bf6-4b94-9ec3-4961858be0ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# select the value column from the Kafka stream\n",
    "#value_df = df.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# print the value column to the console\n",
    "#query = value_df \\\n",
    "#    .writeStream \\\n",
    "#    .outputMode(\"append\") \\\n",
    "#    .format(\"console\") \\\n",
    "#    .start()\n",
    "\n",
    "# wait for the query to terminate\n",
    "#query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39b5628-d9d9-469f-990a-727211c205c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType, TimestampType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_transaction = StructType([\n",
    "    StructField(\"@timestamp\", StringType(), True),\n",
    "    StructField(\"account-id\", LongType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"customer-id\", LongType(), True),\n",
    "    StructField(\"datetime\", StringType(), True),\n",
    "    StructField(\"is_fraud\", StringType(), True),\n",
    "    StructField(\"transaction-id\", LongType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_account = StructType([\n",
    "    StructField(\"customer-id\",  LongType(), True),\n",
    "    StructField(\"account-id\", LongType(), True)\n",
    "])\n",
    "\n",
    "transaction_df = spark.createDataFrame([], schema=schema_transaction)\n",
    "account_df = spark.createDataFrame([], schema=schema_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03e5523-4c73-4511-ad71-d2839806fafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, date_format\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import base64\n",
    "import decimal\n",
    "from time import sleep\n",
    "\n",
    "def getDecimalFromKafka(encoded):\n",
    "    \n",
    "    # Decode the Base64 encoded string and create a BigInteger from it\n",
    "    decoded = decimal.Decimal(int.from_bytes(base64.b64decode(encoded), byteorder='big', signed=False))\n",
    "\n",
    "    # Create a context object with the specified scale\n",
    "    context = decimal.Context(prec=28, rounding=decimal.ROUND_HALF_DOWN)\n",
    "\n",
    "    # Set the scale of the decimal value using the context object\n",
    "    decimal_value = decoded.quantize(decimal.Decimal('.1') ** 3, context=context)\n",
    "\n",
    "    return decimal_value/1000000\n",
    "\n",
    "def write_to_es_transaction(df_t, epoch_id):\n",
    "        \n",
    "    global transaction_df\n",
    "    \n",
    "    row_transaction=df_t.first()\n",
    "    \n",
    "    if(row_transaction):\n",
    "        value_dict_transaction = json.loads(row_transaction.value)\n",
    "        \n",
    "        timestamp = value_dict_transaction['payload']['after']['created_date']/1000\n",
    "        # convert Unix timestamp to a datetime object\n",
    "        dt = datetime.datetime.fromtimestamp(timestamp)\n",
    "        # format datetime object as \"yyyy-mm-dd hh:mm:ss\"\n",
    "        formatted_dt = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        \n",
    "        account_id = value_dict_transaction['payload']['after']['savings_account_id']\n",
    "        # Filter the DataFrame to get rows where \"account-id\" is 12\n",
    "        while account_df.filter(col(\"account-id\") == account_id).count() == 0:\n",
    "            # Wait for 0.1 second before checking again\n",
    "            sleep(0.1)\n",
    "        # Code to execute after the condition becomes true\n",
    "        filtered_account_df = account_df.filter(account_df[\"account-id\"] == account_id)\n",
    "        # Select the \"customer-id\" column from the filtered DataFrame\n",
    "        cutomer_id = filtered_account_df.select(\"customer-id\").collect()[0][0]\n",
    "        \n",
    "        new_row_transaction = spark.createDataFrame([(formatted_dt,\n",
    "                                                      account_id,\n",
    "                                                      float(getDecimalFromKafka(value_dict_transaction['payload']['after']['amount'])),\n",
    "                                                      cutomer_id,\n",
    "                                                      formatted_dt,\n",
    "                                                     # date_format(from_unixtime(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "                                                      'valid',\n",
    "                                                      value_dict_transaction['payload']['after']['id'],\n",
    "                                                     )], schema=schema_transaction)\n",
    "        transaction_df = transaction_df.union(new_row_transaction)\n",
    "        \n",
    "        transaction_df.show()\n",
    "        \n",
    "        transaction_df = transaction_df.filter(\"1 = 0\")\n",
    "        \n",
    "        \n",
    "def write_to_es_account(df_a, epoch_id):\n",
    "    \n",
    "    global account_df\n",
    "    \n",
    "    row_account=df_a.first()\n",
    "    \n",
    "    if(row_account):\n",
    "        value_dict_account = json.loads(row_account.value)\n",
    "        new_row_account= spark.createDataFrame([(value_dict_account['payload']['after']['client_id'],\n",
    "                                                      value_dict_account['payload']['after']['id'],\n",
    "                                                     )], schema=schema_account)\n",
    "        \n",
    "        # Check if new_row_account is already present in acount_df\n",
    "        if account_df.subtract(new_row_account).count() == account_df.count():\n",
    "            # new_row_account does not exist in acount_df, so concatenate the two DataFrames\n",
    "            account_df = account_df.union(new_row_account)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa07dd-f2a7-4be5-a36a-747472e32f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|         @timestamp|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|2023-05-05 15:24:51|         2| 300.0|          2|2023-05-05 15:24:51|   valid|             7|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|         @timestamp|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|2023-05-05 15:28:18|         2| 300.0|          2|2023-05-05 15:28:18|   valid|             8|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|         @timestamp|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|2023-05-05 15:44:49|         2|1444.0|          2|2023-05-05 15:44:49|   valid|            12|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|         @timestamp|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|2023-05-05 17:33:06|         2| 250.0|          2|2023-05-05 17:33:06|   valid|            22|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|         @timestamp|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|2023-05-05 16:25:48|         2| 250.0|          2|2023-05-05 16:25:48|   valid|            19|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|         @timestamp|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "|2023-05-05 17:33:56|         2| 250.0|          2|2023-05-05 17:33:56|   valid|            23|\n",
      "+-------------------+----------+------+-----------+-------------------+--------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the write_to_es function on each micro-batch of data\n",
    "value_df_account = df_account_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_account = value_df_account.writeStream.foreachBatch(write_to_es_account).start()\n",
    "\n",
    "value_df_transaction = df_transaction_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_transaction = value_df_transaction.writeStream.foreachBatch(write_to_es_transaction).start()\n",
    "\n",
    "# Wait for the stream to finish\n",
    "#query_account.awaitTermination()\n",
    "query_transaction.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
