{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9551b6e-b899-471c-b6ef-8cf2d75538e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n",
      "Requirement already satisfied: elasticsearch in /opt/conda/lib/python3.10/site-packages (8.7.0)\n",
      "Requirement already satisfied: elastic-transport<9,>=8 in /opt/conda/lib/python3.10/site-packages (from elasticsearch) (8.4.0)\n",
      "Requirement already satisfied: urllib3<2,>=1.26.2 in /opt/conda/lib/python3.10/site-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.15)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from elastic-transport<9,>=8->elasticsearch) (2022.12.7)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.10/site-packages (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "\n",
    "!pip install elasticsearch\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a38135-4cfd-4fc7-946d-0441ca71a3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.elasticsearch:elasticsearch-spark-30_2.12:7.16.2,com.fasterxml.jackson.module:jackson-module-scala_2.12:2.13.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4473cf6a-b786-4972-bf5a-b212e194b2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName(\"kafka_elastic_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbaf5ab-b77f-4c10-81e9-f3e93198f5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a Kafka stream for transaction\n",
    "df_transaction_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account_transaction\") \\\n",
    "    .load()\n",
    "\n",
    "# create a Kafka stream for account\n",
    "df_account_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39b5628-d9d9-469f-990a-727211c205c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType, TimestampType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_transaction = StructType([\n",
    "    StructField(\"account-id\", LongType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"customer-id\", LongType(), True),\n",
    "    StructField(\"datetime\", StringType(), True),\n",
    "    StructField(\"is_fraud\", StringType(), True),\n",
    "    StructField(\"transaction-id\", LongType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"@timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_account = StructType([\n",
    "    StructField(\"customer-id\",  LongType(), True),\n",
    "    StructField(\"account-id\", LongType(), True)\n",
    "])\n",
    "\n",
    "account_df = spark.createDataFrame([], schema=schema_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46231f19-f26b-456d-9ed5-6bb974e33a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import RequestError\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Define the Elasticsearch index name\n",
    "es_index = \"transactions_index\"\n",
    "es_port = 9200\n",
    "es_host = \"elasticsearch\"\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the username and password\n",
    "username = os.environ.get(\"USERNAME\")\n",
    "password = os.environ.get(\"PASSWORD\")\n",
    "\n",
    "# Create an Elasticsearch client\n",
    "es = Elasticsearch(\n",
    "    [{\"host\": es_host, \"port\": es_port, \"scheme\": \"http\"}],\n",
    "     basic_auth=(username, password)\n",
    ")\n",
    "\n",
    "# Create a mapping for the Elasticsearch index\n",
    "# Define the index mapping\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"@timestamp\": {\"type\": \"date\"},\n",
    "            \"account-id\": {\"type\": \"long\"},\n",
    "            \"amount\": {\"type\": \"double\"},\n",
    "            \"customer-id\": {\"type\": \"long\"},\n",
    "            \"datetime\": {\"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss\"},\n",
    "            \"is_fraud\": {\"type\": \"keyword\"},\n",
    "            \"transaction-id\": {\"type\": \"long\"},\n",
    "            \"type\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if the index exists, and create it if it does not\n",
    "if not es.indices.exists(index=es_index):\n",
    "    try:\n",
    "        # Create the Elasticsearch index if it doesn't exist\n",
    "        es.indices.create(index=es_index, body=mapping)\n",
    "    except RequestError as e:\n",
    "        print(f\"Index creation failed: {e}\")\n",
    "        exit(1)\n",
    "    print(f\"Created Elasticsearch index '{es_index}'\")\n",
    "    \n",
    "# Write the streaming DataFrame to Elasticsearch\n",
    "def write_to_es(es_df):\n",
    "    es_df.show()\n",
    "    es_df.write \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", es_host) \\\n",
    "    .option(\"es.port\", es_port) \\\n",
    "    .option(\"es.net.http.auth.user\", username) \\\n",
    "    .option(\"es.net.http.auth.pass\", password) \\\n",
    "    .option(\"es.resource\", es_index) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b03e5523-4c73-4511-ad71-d2839806fafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, date_format\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import base64\n",
    "import decimal\n",
    "from time import sleep\n",
    "\n",
    "def getDecimalFromKafka(encoded):\n",
    "    \n",
    "    # Decode the Base64 encoded string and create a BigInteger from it\n",
    "    decoded = decimal.Decimal(int.from_bytes(base64.b64decode(encoded), byteorder='big', signed=False))\n",
    "\n",
    "    # Create a context object with the specified scale\n",
    "    context = decimal.Context(prec=28, rounding=decimal.ROUND_HALF_DOWN)\n",
    "\n",
    "    # Set the scale of the decimal value using the context object\n",
    "    decimal_value = decoded.quantize(decimal.Decimal('.1') ** 3, context=context)\n",
    "\n",
    "    return decimal_value/1000000\n",
    "\n",
    "def write_to_es_transaction(df_t, epoch_id):\n",
    "    \n",
    "    row_transactions = df_t.collect()\n",
    "    \n",
    "    for row_transaction in row_transactions:\n",
    "    \n",
    "      # if(row_transaction):\n",
    "            value_dict_transaction = json.loads(row_transaction.value)\n",
    "            \n",
    "            if value_dict_transaction['payload']['before'] == None :\n",
    "                \n",
    "                timestamp = value_dict_transaction['payload']['after']['created_date']/1000\n",
    "                # convert Unix timestamp to a datetime object\n",
    "                dt = datetime.datetime.fromtimestamp(timestamp)\n",
    "                # format datetime object as \"yyyy-mm-dd hh:mm:ss\"\n",
    "                formatted_date = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                formatted_date_es = dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "        \n",
    "                account_id = value_dict_transaction['payload']['after']['savings_account_id']\n",
    "        \n",
    "                while account_df.filter(col(\"account-id\") == account_id).count() == 0:\n",
    "                    # Wait for 0.1 second before checking again\n",
    "                    sleep(0.1)\n",
    "        \n",
    "                # Code to execute after the condition becomes true\n",
    "                # Filter the DataFrame to get rows where \"account-id\" is in account_df[\"account-id\"]\n",
    "                filtered_account_df = account_df.filter(account_df[\"account-id\"] == account_id)\n",
    "                # Select the \"customer-id\" column from the filtered DataFrame\n",
    "                cutomer_id = filtered_account_df.select(\"customer-id\").collect()[0][0]\n",
    "                op_type = \"DEBIT\" if value_dict_transaction['payload']['after']['transaction_type_enum'] == 2 else \"CREDIT\"  \n",
    "                \n",
    "                new_row_transaction = spark.createDataFrame([(account_id,\n",
    "                                                      float(getDecimalFromKafka(value_dict_transaction['payload']['after']['amount'])),\n",
    "                                                      cutomer_id,\n",
    "                                                      formatted_date,\n",
    "                                                     # date_format(from_unixtime(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "                                                      'valid',#-----test after will be predected by ML\n",
    "                                                      value_dict_transaction['payload']['after']['id'],\n",
    "                                                      op_type,\n",
    "                                                      formatted_date_es,\n",
    "                                                     )], schema=schema_transaction)\n",
    "                        \n",
    "                write_to_es(new_row_transaction)\n",
    "                    \n",
    "        \n",
    "def write_to_es_account(df_a, epoch_id):\n",
    "    \n",
    "    global account_df\n",
    "        \n",
    "    row_accounts = df_a.collect()\n",
    "    \n",
    "    for row_account in row_accounts :\n",
    "    \n",
    "       #if(row_account):\n",
    "            value_dict_account = json.loads(row_account.value)\n",
    "            new_row_account= spark.createDataFrame([(value_dict_account['payload']['after']['client_id'],\n",
    "                                                      value_dict_account['payload']['after']['id'],\n",
    "                                                     )], schema=schema_account)\n",
    "        \n",
    "            # Check if new_row_account is already present in acount_df\n",
    "            if account_df.subtract(new_row_account).count() == account_df.count():\n",
    "                # new_row_account does not exist in acount_df, so concatenate the two DataFrames\n",
    "                account_df = account_df.union(new_row_account)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efa07dd-f2a7-4be5-a36a-747472e32f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+-------------------+--------+--------------+------+--------------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|  type|          @timestamp|\n",
      "+----------+------+-----------+-------------------+--------+--------------+------+--------------------+\n",
      "|         4| 250.0|          2|2023-05-21 21:23:37|   valid|           107|CREDIT|2023-05-21T21:23:...|\n",
      "+----------+------+-----------+-------------------+--------+--------------+------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m query_transaction \u001b[38;5;241m=\u001b[39m value_df_transaction\u001b[38;5;241m.\u001b[39mwriteStream\u001b[38;5;241m.\u001b[39mforeachBatch(write_to_es_transaction)\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Wait for the stream to finish\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#query_account.awaitTermination()\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mquery_transaction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming.py:107\u001b[0m, in \u001b[0;36mStreamingQuery.awaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsq\u001b[38;5;241m.\u001b[39mawaitTermination(\u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Call the write_to_es function on each micro-batch of data\n",
    "value_df_account = df_account_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_account = value_df_account.writeStream.foreachBatch(write_to_es_account).start()\n",
    "\n",
    "value_df_transaction = df_transaction_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_transaction = value_df_transaction.writeStream.foreachBatch(write_to_es_transaction).start()\n",
    "\n",
    "# Wait for the stream to finish\n",
    "#query_account.awaitTermination()\n",
    "query_transaction.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
