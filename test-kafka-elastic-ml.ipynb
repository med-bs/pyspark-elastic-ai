{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9551b6e-b899-471c-b6ef-8cf2d75538e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n",
      "Collecting elasticsearch\n",
      "  Downloading elasticsearch-8.7.0-py3-none-any.whl (387 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.9/387.9 kB\u001b[0m \u001b[31m626.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting elastic-transport<9,>=8\n",
      "  Downloading elastic_transport-8.4.0-py3-none-any.whl (59 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m486.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3<2,>=1.26.2 in /opt/conda/lib/python3.10/site-packages (from elastic-transport<9,>=8->elasticsearch) (1.26.15)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from elastic-transport<9,>=8->elasticsearch) (2022.12.7)\n",
      "Installing collected packages: elastic-transport, elasticsearch\n",
      "Successfully installed elastic-transport-8.4.0 elasticsearch-8.7.0\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "\n",
    "!pip install elasticsearch\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a38135-4cfd-4fc7-946d-0441ca71a3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.elasticsearch:elasticsearch-spark-30_2.12:7.16.2,com.fasterxml.jackson.module:jackson-module-scala_2.12:2.13.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4473cf6a-b786-4972-bf5a-b212e194b2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName(\"kafka_elastic_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbaf5ab-b77f-4c10-81e9-f3e93198f5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a Kafka stream for transaction\n",
    "df_transaction_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account_transaction\") \\\n",
    "    .load()\n",
    "\n",
    "# create a Kafka stream for account\n",
    "df_account_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39b5628-d9d9-469f-990a-727211c205c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_transaction = StructType([\n",
    "    StructField(\"account-id\", LongType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"customer-id\", LongType(), True),\n",
    "    StructField(\"datetime\", StringType(), True),\n",
    "    StructField(\"is_fraud\", StringType(), True),\n",
    "    StructField(\"transaction-id\", LongType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"@timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_account = StructType([\n",
    "    StructField(\"customer-id\",  LongType(), True),\n",
    "    StructField(\"account-id\", LongType(), True)\n",
    "])\n",
    "\n",
    "account_df = spark.createDataFrame([], schema=schema_account)\n",
    "\n",
    "dic_trans = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46231f19-f26b-456d-9ed5-6bb974e33a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import RequestError\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Define the Elasticsearch index name\n",
    "es_index = \"transactions_index\"\n",
    "es_port = 9200\n",
    "es_host = \"elasticsearch\"\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the username and password\n",
    "username = os.environ.get(\"USERNAME\")\n",
    "password = os.environ.get(\"PASSWORD\")\n",
    "\n",
    "# Create an Elasticsearch client\n",
    "es = Elasticsearch(\n",
    "    [{\"host\": es_host, \"port\": es_port, \"scheme\": \"http\"}],\n",
    "     basic_auth=(username, password)\n",
    ")\n",
    "\n",
    "# Create a mapping for the Elasticsearch index\n",
    "# Define the index mapping\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"@timestamp\": {\"type\": \"date\"},\n",
    "            \"account-id\": {\"type\": \"long\"},\n",
    "            \"amount\": {\"type\": \"double\"},\n",
    "            \"customer-id\": {\"type\": \"long\"},\n",
    "            \"datetime\": {\"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss\"},\n",
    "            \"is_fraud\": {\"type\": \"keyword\"},\n",
    "            \"transaction-id\": {\"type\": \"long\"},\n",
    "            \"type\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if the index exists, and create it if it does not\n",
    "if not es.indices.exists(index=es_index):\n",
    "    try:\n",
    "        # Create the Elasticsearch index if it doesn't exist\n",
    "        es.indices.create(index=es_index, body=mapping)\n",
    "    except RequestError as e:\n",
    "        print(f\"Index creation failed: {e}\")\n",
    "        exit(1)\n",
    "    print(f\"Created Elasticsearch index '{es_index}'\")\n",
    "    \n",
    "# Write the streaming DataFrame to Elasticsearch\n",
    "def write_to_es(es_df):\n",
    "    es_df.show()\n",
    "    es_df.write \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", es_host) \\\n",
    "    .option(\"es.port\", es_port) \\\n",
    "    .option(\"es.net.http.auth.user\", username) \\\n",
    "    .option(\"es.net.http.auth.pass\", password) \\\n",
    "    .option(\"es.resource\", es_index) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dd2635f-1b12-437d-858f-b245230a0cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def is_night(datetime_str):\n",
    "    datetime_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    datetime_obj = dt.strptime(datetime_str, datetime_format)\n",
    "    hour = datetime_obj.hour\n",
    "    \n",
    "    if hour >= 22 or hour < 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_weekend(datetime_str):\n",
    "    datetime_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    datetime_obj = dt.strptime(datetime_str, datetime_format)\n",
    "    weekday = datetime_obj.weekday()\n",
    "    \n",
    "    if weekday >= 5:  # 5 and 6 correspond to Saturday and Sunday\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d79b8663-d3b5-45e3-90c9-6854066f1dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mllib ia model\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "lr_model = LogisticRegressionModel.load(\"lr_ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebb5d36e-7466-45df-b04d-760879ceb234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12 µs, sys: 4 µs, total: 16 µs\n",
      "Wall time: 19.6 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def predict_isfraude(dic_trans):\n",
    "    \n",
    "    dense_vector = Vectors.dense([\n",
    "        dic_trans['amount'],\n",
    "        1500.0, 5000.0, 3000.0, 2500.0,\n",
    "        1.0, 2.0, 3.0, 4.0,\n",
    "        7000.0, 6000.0, 4500.0, 2775.0,\n",
    "        2.0, 4.0, 5.0, 7.0,\n",
    "        dic_trans['in_weekend'], dic_trans['at_night']\n",
    "    ])\n",
    "    data_vect = spark.createDataFrame([(dense_vector, ), ], ['features'])\n",
    "    is_fraude = lr_model.transform(data_vect).select(['prediction']).collect()[0][0]\n",
    "    \n",
    "    return \"valid\" if is_fraude == 0.0 else \"fraudulent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b03e5523-4c73-4511-ad71-d2839806fafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, date_format\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import base64\n",
    "import decimal\n",
    "from time import sleep\n",
    "\n",
    "def getDecimalFromKafka(encoded):\n",
    "    \n",
    "    # Decode the Base64 encoded string and create a BigInteger from it\n",
    "    decoded = decimal.Decimal(int.from_bytes(base64.b64decode(encoded), byteorder='big', signed=False))\n",
    "\n",
    "    # Create a context object with the specified scale\n",
    "    context = decimal.Context(prec=28, rounding=decimal.ROUND_HALF_DOWN)\n",
    "\n",
    "    # Set the scale of the decimal value using the context object\n",
    "    decimal_value = decoded.quantize(decimal.Decimal('.1') ** 3, context=context)\n",
    "\n",
    "    return decimal_value/1000000\n",
    "\n",
    "def write_to_es_transaction(df_t, epoch_id):\n",
    "    \n",
    "    row_transactions = df_t.collect()\n",
    "    \n",
    "    for row_transaction in row_transactions:\n",
    "    \n",
    "      # if(row_transaction):\n",
    "            value_dict_transaction = json.loads(row_transaction.value)\n",
    "            \n",
    "            if value_dict_transaction['payload']['before'] == None :\n",
    "                \n",
    "                timestamp = value_dict_transaction['payload']['after']['created_date']/1000\n",
    "                # convert Unix timestamp to a datetime object\n",
    "                dt = datetime.datetime.fromtimestamp(timestamp)\n",
    "                # format datetime object as \"yyyy-mm-dd hh:mm:ss\"\n",
    "                formatted_date = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                formatted_date_es = dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "        \n",
    "                account_id = value_dict_transaction['payload']['after']['savings_account_id']\n",
    "        \n",
    "                while account_df.filter(col(\"account-id\") == account_id).count() == 0:\n",
    "                    # Wait for 0.1 second before checking again\n",
    "                    sleep(0.1)\n",
    "        \n",
    "                # Code to execute after the condition becomes true\n",
    "                # Filter the DataFrame to get rows where \"account-id\" is in account_df[\"account-id\"]\n",
    "                filtered_account_df = account_df.filter(account_df[\"account-id\"] == account_id)\n",
    "                # Select the \"customer-id\" column from the filtered DataFrame\n",
    "                cutomer_id = filtered_account_df.select(\"customer-id\").collect()[0][0]\n",
    "                op_type = \"DEBIT\" if value_dict_transaction['payload']['after']['transaction_type_enum'] == 2 else \"CREDIT\"  \n",
    "                \n",
    "                dic_trans['amount'] = float(getDecimalFromKafka(value_dict_transaction['payload']['after']['amount']))\n",
    "                dic_trans['in_weekend'] = is_night(formatted_date)\n",
    "                dic_trans['at_night'] = is_weekend(formatted_date)\n",
    "                \n",
    "                new_row_transaction = spark.createDataFrame([(account_id,\n",
    "                                                      dic_trans['amount'],\n",
    "                                                      cutomer_id,\n",
    "                                                      formatted_date,\n",
    "                                                     # date_format(from_unixtime(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "                                                      predict_isfraude(dic_trans),#-----test after will be predected by ML\n",
    "                                                      value_dict_transaction['payload']['after']['id'],\n",
    "                                                      op_type,\n",
    "                                                      formatted_date_es,\n",
    "                                                     )], schema=schema_transaction)\n",
    "                        \n",
    "                write_to_es(new_row_transaction)\n",
    "                    \n",
    "        \n",
    "def write_to_es_account(df_a, epoch_id):\n",
    "    \n",
    "    global account_df\n",
    "        \n",
    "    row_accounts = df_a.collect()\n",
    "    \n",
    "    for row_account in row_accounts :\n",
    "    \n",
    "       #if(row_account):\n",
    "            value_dict_account = json.loads(row_account.value)\n",
    "            new_row_account= spark.createDataFrame([(value_dict_account['payload']['after']['client_id'],\n",
    "                                                      value_dict_account['payload']['after']['id'],\n",
    "                                                     )], schema=schema_account)\n",
    "        \n",
    "            # Check if new_row_account is already present in acount_df\n",
    "            if account_df.subtract(new_row_account).count() == account_df.count():\n",
    "                # new_row_account does not exist in acount_df, so concatenate the two DataFrames\n",
    "                account_df = account_df.union(new_row_account)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa07dd-f2a7-4be5-a36a-747472e32f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id| type|          @timestamp|\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "|         4| 310.0|          2|2023-05-22 13:42:42|   valid|           115|DEBIT|2023-05-22T13:42:...|\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id| type|          @timestamp|\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "|         4| 310.0|          2|2023-05-22 13:42:42|   valid|           115|DEBIT|2023-05-22T13:42:...|\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "\n",
      "\n",
      "+----------+-------+-----------+-------------------+----------+--------------+------+--------------------+\n",
      "|account-id| amount|customer-id|           datetime|  is_fraud|transaction-id|  type|          @timestamp|\n",
      "+----------+-------+-----------+-------------------+----------+--------------+------+--------------------+\n",
      "|         4|14444.0|          2|2023-05-22 13:43:19|fraudulent|           116|CREDIT|2023-05-22T13:43:...|\n",
      "+----------+-------+-----------+-------------------+----------+--------------+------+--------------------+\n",
      "\n",
      "+----------+-------+-----------+-------------------+----------+--------------+------+--------------------+\n",
      "|account-id| amount|customer-id|           datetime|  is_fraud|transaction-id|  type|          @timestamp|\n",
      "+----------+-------+-----------+-------------------+----------+--------------+------+--------------------+\n",
      "|         4|14444.0|          2|2023-05-22 13:43:19|fraudulent|           116|CREDIT|2023-05-22T13:43:...|\n",
      "+----------+-------+-----------+-------------------+----------+--------------+------+--------------------+\n",
      "\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id| type|          @timestamp|\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "|         4| 565.0|          2|2023-05-22 15:29:32|   valid|           117|DEBIT|2023-05-22T15:29:...|\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id| type|          @timestamp|\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "|         4| 565.0|          2|2023-05-22 15:29:32|   valid|           117|DEBIT|2023-05-22T15:29:...|\n",
      "+----------+------+-----------+-------------------+--------+--------------+-----+--------------------+\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the write_to_es function on each micro-batch of data\n",
    "value_df_account = df_account_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_account = value_df_account.writeStream.foreachBatch(write_to_es_account).start()\n",
    "\n",
    "value_df_transaction = df_transaction_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_transaction = value_df_transaction.writeStream.foreachBatch(write_to_es_transaction).start()\n",
    "\n",
    "# Wait for the stream to finish\n",
    "#query_account.awaitTermination()\n",
    "query_transaction.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
