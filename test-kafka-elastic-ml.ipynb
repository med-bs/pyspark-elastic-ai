{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9551b6e-b899-471c-b6ef-8cf2d75538e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.3.2\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)\n",
    "\n",
    "#!pip install elasticsearch\n",
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38a38135-4cfd-4fc7-946d-0441ca71a3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\n",
    "    \"PYSPARK_SUBMIT_ARGS\"\n",
    "] = \"--packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.2,org.elasticsearch:elasticsearch-spark-30_2.12:7.16.2,com.fasterxml.jackson.module:jackson-module-scala_2.12:2.13.0 pyspark-shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4473cf6a-b786-4972-bf5a-b212e194b2f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# create a Spark session\n",
    "spark = SparkSession.builder.appName(\"kafka_elastic_test\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbaf5ab-b77f-4c10-81e9-f3e93198f5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create a Kafka stream for transaction\n",
    "df_transaction_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account_transaction\") \\\n",
    "    .option(\"group.id\", \"1\") \\\n",
    "    .load()\n",
    "\n",
    "# create a Kafka stream for account\n",
    "df_account_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"dbserver1.fineract_default.m_savings_account\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b39b5628-d9d9-469f-990a-727211c205c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_transaction = StructType([\n",
    "    StructField(\"account-id\", LongType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"customer-id\", LongType(), True),\n",
    "    StructField(\"datetime\", StringType(), True),\n",
    "    StructField(\"is_fraud\", StringType(), True),\n",
    "    StructField(\"transaction-id\", LongType(), True),\n",
    "    StructField(\"type\", StringType(), True),\n",
    "    StructField(\"@timestamp\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "schema_account = StructType([\n",
    "    StructField(\"customer_id\",  LongType(), True),\n",
    "    StructField(\"account_id\", LongType(), True)\n",
    "])\n",
    "\n",
    "account_df = spark.createDataFrame([(-1,-1),], schema=schema_account)\n",
    "\n",
    "dic_trans = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46231f19-f26b-456d-9ed5-6bb974e33a41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import RequestError\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Define the Elasticsearch index name\n",
    "es_index = \"transactions_index\"\n",
    "es_port = 9200\n",
    "es_host = \"elasticsearch\"\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the username and password\n",
    "username = os.environ.get(\"USERNAME\")\n",
    "password = os.environ.get(\"PASSWORD\")\n",
    "\n",
    "# Create an Elasticsearch client\n",
    "es = Elasticsearch(\n",
    "    [{\"host\": es_host, \"port\": es_port, \"scheme\": \"http\"}],\n",
    "     basic_auth=(username, password)\n",
    ")\n",
    "\n",
    "# Create a mapping for the Elasticsearch index\n",
    "# Define the index mapping\n",
    "mapping = {\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"@timestamp\": {\"type\": \"date\"},\n",
    "            \"account-id\": {\"type\": \"long\"},\n",
    "            \"amount\": {\"type\": \"double\"},\n",
    "            \"customer-id\": {\"type\": \"long\"},\n",
    "            \"datetime\": {\"type\": \"date\", \"format\": \"yyyy-MM-dd HH:mm:ss\"},\n",
    "            \"is_fraud\": {\"type\": \"keyword\"},\n",
    "            \"transaction-id\": {\"type\": \"long\"},\n",
    "            \"type\": {\"type\": \"keyword\"},\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Check if the index exists, and create it if it does not\n",
    "if not es.indices.exists(index=es_index):\n",
    "    try:\n",
    "        # Create the Elasticsearch index if it doesn't exist\n",
    "        es.indices.create(index=es_index, body=mapping)\n",
    "    except RequestError as e:\n",
    "        print(f\"Index creation failed: {e}\")\n",
    "        exit(1)\n",
    "    print(f\"Created Elasticsearch index '{es_index}'\")\n",
    "    \n",
    "# Write the streaming DataFrame to Elasticsearch\n",
    "def write_to_es(es_df):\n",
    "    es_df.show()\n",
    "    es_df.write \\\n",
    "    .format(\"org.elasticsearch.spark.sql\") \\\n",
    "    .option(\"es.nodes\", es_host) \\\n",
    "    .option(\"es.port\", es_port) \\\n",
    "    .option(\"es.net.http.auth.user\", username) \\\n",
    "    .option(\"es.net.http.auth.pass\", password) \\\n",
    "    .option(\"es.resource\", es_index) \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd2635f-1b12-437d-858f-b245230a0cc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "\n",
    "def is_night(datetime_str):\n",
    "    datetime_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    datetime_obj = dt.strptime(datetime_str, datetime_format)\n",
    "    hour = datetime_obj.hour\n",
    "    \n",
    "    if hour >= 22 or hour < 6:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_weekend(datetime_str):\n",
    "    datetime_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    datetime_obj = dt.strptime(datetime_str, datetime_format)\n",
    "    weekday = datetime_obj.weekday()\n",
    "    \n",
    "    if weekday >= 5:  # 5 and 6 correspond to Saturday and Sunday\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc1887de-396d-4e3c-8310-bf7de878d0bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import redis\n",
    "import datetime\n",
    "\n",
    "# Create a Redis connection\n",
    "redis_host = \"redis\"\n",
    "redis_port = 6379\n",
    "redis_db_tr = 0\n",
    "redis_db_acc = 1\n",
    "\n",
    "redis_client = redis.Redis(host=redis_host, port=redis_port, db= redis_db_tr)\n",
    "\n",
    "redis_client_acc = redis.Redis(host=redis_host, port=redis_port, db= redis_db_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e75abdc1-1c70-4a27-9bf3-5294b4592ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def add_transaction_to_history(key, data):\n",
    "        # Serialize the dictionary into a JSON string\n",
    "        data_json = json.dumps(data)\n",
    "\n",
    "        # Save the serialized data in Redis\n",
    "        redis_client.set(key, data_json)\n",
    "        \n",
    "        # Set the expiration time for the key\n",
    "        redis_client.expire(key, 7889229) # 3 months\n",
    "        \n",
    "        \n",
    "def add_account_to_cache(key, data):\n",
    "        # Serialize the dictionary into a JSON string\n",
    "        data_json = json.dumps(data)\n",
    "\n",
    "        # Save the serialized data in Redis\n",
    "        redis_client_acc.set(key, data_json)\n",
    "        \n",
    "        # Set the expiration time for the key\n",
    "        redis_client_acc.expire(key, 604800) # 1 week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a07e8743-e681-404a-89a0-ab3deed28dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_transactions_history():\n",
    "    \n",
    "    # Get all keys in Redis\n",
    "    all_keys = redis_client.keys(\"*\")\n",
    "\n",
    "    # Retrieve values for each key\n",
    "    data = {}\n",
    "    for key in all_keys:\n",
    "        value = redis_client.get(key)\n",
    "        data[key.decode(\"utf-8\")] = json.loads(value.decode(\"utf-8\"))\n",
    "\n",
    "    # Convert list of dictionaries to RDD\n",
    "    rdd = spark.sparkContext.parallelize(list(data.values()))\n",
    "\n",
    "    # Create DataFrame from RDD\n",
    "    return spark.createDataFrame(rdd)\n",
    "\n",
    "def get_account_from_cache(key):\n",
    "    \n",
    "    data = {}\n",
    "    # Retrieve values for each key\n",
    "    value = redis_client_acc.get(key)\n",
    "    data[key] = json.loads(value.decode(\"utf-8\"))\n",
    "\n",
    "    # Convert list of dictionaries to RDD\n",
    "    rdd = spark.sparkContext.parallelize(list(data.values()))\n",
    "\n",
    "    # Create DataFrame from RDD\n",
    "    return spark.createDataFrame(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41579d8e-2f8b-4a15-a1fb-17aa3bd4ab56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count, current_date, date_sub\n",
    "from pyspark.sql import Window\n",
    "\n",
    "def get_ml_transaction_input_byIds(account_id, customer_id):\n",
    "    dict_ml = {}\n",
    "    \n",
    "    # get transaction from redis\n",
    "    trans_df = get_transactions_history()\n",
    "\n",
    "    # Filter transactions for the given customer_id\n",
    "    customer_filtered_df = trans_df.filter(col(\"customer_id\") == customer_id)\n",
    "\n",
    "    # Calculate customer_id average amount for different time periods\n",
    "    dict_ml['customer_id_avrge_amount_1day'] = customer_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 1)).select(avg(\"amount\")).collect()[0][0]\n",
    "    dict_ml['customer_id_avrge_amount_1week'] = customer_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 7)).select(avg(\"amount\")).collect()[0][0]\n",
    "    dict_ml['customer_id_avrge_amount_1month'] = customer_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 30)).select(avg(\"amount\")).collect()[0][0]\n",
    "    dict_ml['customer_id_avrge_amount_3month'] = customer_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 90)).select(avg(\"amount\")).collect()[0][0]\n",
    "\n",
    "    # Calculate customer_id transaction counts for different time periods\n",
    "    dict_ml['customer_id_count_1day'] = customer_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 1)).count()\n",
    "    dict_ml['customer_id_count_1week'] = customer_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 7)).count()\n",
    "    dict_ml['customer_id_count_1month'] = customer_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 30)).count()\n",
    "    dict_ml['customer_id_count_3month'] = customer_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 90)).count()\n",
    "\n",
    "    # Filter transactions for the given account_id\n",
    "    account_filtered_df = trans_df.filter(col(\"account_id\") == account_id)\n",
    "\n",
    "    # Calculate account_id average amount for different time periods\n",
    "    dict_ml['account_id_avrge_amount_1day'] = account_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 1)).select(avg(\"amount\")).collect()[0][0]\n",
    "    dict_ml['account_id_avrge_amount_1week'] = account_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 7)).select(avg(\"amount\")).collect()[0][0]\n",
    "    dict_ml['account_id_avrge_amount_1month'] = account_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 30)).select(avg(\"amount\")).collect()[0][0]\n",
    "    dict_ml['account_id_avrge_amount_3month'] = account_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 90)).select(avg(\"amount\")).collect()[0][0]\n",
    "\n",
    "    # Calculate account_id transaction counts for different time periods\n",
    "    dict_ml['account_id_count_1day'] = account_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 1)).count()\n",
    "    dict_ml['account_id_count_1week'] = account_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 7)).count()\n",
    "    dict_ml['account_id_count_1month'] = account_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 30)).count()\n",
    "    dict_ml['account_id_count_3month'] = account_filtered_df.filter(col(\"datetime\") >= date_sub(current_date(), 90)).count()\n",
    "    \n",
    "    return dict_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d79b8663-d3b5-45e3-90c9-6854066f1dc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mllib ia model\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "lr_model = LogisticRegressionModel.load(\"lr_ml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebb5d36e-7466-45df-b04d-760879ceb234",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14 µs, sys: 3 µs, total: 17 µs\n",
      "Wall time: 20.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "def predict_isfraude(dic_trans):\n",
    "    \n",
    "    # get input variable for ml\n",
    "    ml_input_var = get_ml_transaction_input_byIds( dic_trans['account_id'], dic_trans['customer_id'])\n",
    "    \n",
    "    # create the feature vector\n",
    "    dense_vector = Vectors.dense([\n",
    "        dic_trans['amount'],\n",
    "        ml_input_var['customer_id_avrge_amount_1day'],\n",
    "        ml_input_var['customer_id_avrge_amount_1week'], \n",
    "        ml_input_var['customer_id_avrge_amount_1month'],\n",
    "        ml_input_var['customer_id_avrge_amount_3month'],\n",
    "        ml_input_var['customer_id_count_1day'],\n",
    "        ml_input_var['customer_id_count_1week'],\n",
    "        ml_input_var['customer_id_count_1month'],\n",
    "        ml_input_var['customer_id_count_3month'],\n",
    "        ml_input_var['account_id_avrge_amount_1day'],\n",
    "        ml_input_var['account_id_avrge_amount_1week'],\n",
    "        ml_input_var['account_id_avrge_amount_1month'],\n",
    "        ml_input_var['account_id_avrge_amount_3month'],\n",
    "        ml_input_var['account_id_count_1day'],\n",
    "        ml_input_var['account_id_count_1week'],\n",
    "        ml_input_var['account_id_count_1month'],\n",
    "        ml_input_var['account_id_count_3month'],\n",
    "        dic_trans['in_weekend'], dic_trans['at_night']\n",
    "    ])\n",
    "    data_vect = spark.createDataFrame([(dense_vector, ), ], ['features'])\n",
    "    is_fraude = lr_model.transform(data_vect).select(['prediction']).collect()[0][0]\n",
    "    \n",
    "    return \"valid\" if is_fraude == 0.0 else \"fraudulent\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b03e5523-4c73-4511-ad71-d2839806fafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_unixtime, date_format\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import base64\n",
    "import decimal\n",
    "from time import sleep\n",
    "\n",
    "def getDecimalFromKafka(encoded):\n",
    "    \n",
    "    # Decode the Base64 encoded string and create a BigInteger from it\n",
    "    decoded = decimal.Decimal(int.from_bytes(base64.b64decode(encoded), byteorder='big', signed=False))\n",
    "\n",
    "    # Create a context object with the specified scale\n",
    "    context = decimal.Context(prec=28, rounding=decimal.ROUND_HALF_DOWN)\n",
    "\n",
    "    # Set the scale of the decimal value using the context object\n",
    "    decimal_value = decoded.quantize(decimal.Decimal('.1') ** 3, context=context)\n",
    "\n",
    "    return decimal_value/1000000\n",
    "\n",
    "def write_to_es_transaction(df_t, epoch_id):\n",
    "    \n",
    "    global account_df\n",
    "    \n",
    "    row_transactions = df_t.collect()\n",
    "    \n",
    "    for row_transaction in row_transactions:\n",
    "    \n",
    "            value_dict_transaction = json.loads(row_transaction.value)\n",
    "            \n",
    "            if value_dict_transaction['payload']['before'] == None :\n",
    "                \n",
    "                timestamp = value_dict_transaction['payload']['after']['created_date']/1000\n",
    "                # convert Unix timestamp to a datetime object\n",
    "                dt = datetime.datetime.fromtimestamp(timestamp)\n",
    "                # format datetime object as \"yyyy-mm-dd hh:mm:ss\"\n",
    "                dic_trans['datetime'] = dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                formatted_date_es = dt.strftime(\"%Y-%m-%dT%H:%M:%S.%f%z\")\n",
    "        \n",
    "                dic_trans['account_id'] = value_dict_transaction['payload']['after']['savings_account_id']\n",
    "        \n",
    "                while account_df.filter(col(\"account_id\") == dic_trans['account_id']).count() == 0:\n",
    "                    # Wait for 0.01 second before checking again\n",
    "                    sleep(0.01)\n",
    "                    account_df = get_account_from_cache(dic_trans['account_id'])\n",
    "        \n",
    "                # Code to execute after the condition becomes true\n",
    "                # Filter the DataFrame to get rows where \"account-id\" is in account_df[\"account-id\"]\n",
    "                filtered_account_df = account_df.filter(account_df[\"account_id\"] == dic_trans['account_id'])\n",
    "                # Select the \"customer-id\" column from the filtered DataFrame\n",
    "                dic_trans['customer_id'] = filtered_account_df.select(\"customer_id\").collect()[0][0]\n",
    "                \n",
    "                op_type = \"DEBIT\" if value_dict_transaction['payload']['after']['transaction_type_enum'] == 2 else \"CREDIT\"  \n",
    "                \n",
    "                dic_trans['amount'] = float(getDecimalFromKafka(value_dict_transaction['payload']['after']['amount']))\n",
    "                \n",
    "                # save this transacton in redis\n",
    "                add_transaction_to_history(value_dict_transaction['payload']['after']['id'], dic_trans)\n",
    "                \n",
    "                dic_trans['in_weekend'] = is_night(dic_trans['datetime'])\n",
    "                dic_trans['at_night'] = is_weekend(dic_trans['datetime'])\n",
    "                \n",
    "                new_row_transaction = spark.createDataFrame([(dic_trans['account_id'],\n",
    "                                                      dic_trans['amount'],\n",
    "                                                      dic_trans['customer_id'],\n",
    "                                                      dic_trans['datetime'],\n",
    "                                                      predict_isfraude(dic_trans),#-----test after will be predected by ML\n",
    "                                                      value_dict_transaction['payload']['after']['id'],\n",
    "                                                      op_type,\n",
    "                                                      formatted_date_es,\n",
    "                                                     )], schema=schema_transaction)\n",
    "                        \n",
    "                write_to_es(new_row_transaction)\n",
    "                    \n",
    "        \n",
    "def write_to_es_account(df_a, epoch_id):\n",
    "    global account_df\n",
    "        \n",
    "    row_accounts = df_a.collect()\n",
    "    \n",
    "    for row_account in row_accounts :\n",
    "    \n",
    "            dict_account = {}\n",
    "            value_dict_account = json.loads(row_account.value)\n",
    "            dict_account['customer_id'] = value_dict_account['payload']['after']['client_id']\n",
    "            dict_account['account_id'] = value_dict_account['payload']['after']['id']\n",
    "        \n",
    "            # add account to cache\n",
    "            add_account_to_cache(dict_account['account_id'], dict_account)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa07dd-f2a7-4be5-a36a-747472e32f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----------+-------------------+--------+--------------+------+--------------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|  type|          @timestamp|\n",
      "+----------+------+-----------+-------------------+--------+--------------+------+--------------------+\n",
      "|         4| 300.0|          2|2023-05-25 00:38:13|   valid|           148|CREDIT|2023-05-25T00:38:...|\n",
      "+----------+------+-----------+-------------------+--------+--------------+------+--------------------+\n",
      "\n",
      "+----------+------+-----------+-------------------+--------+--------------+------+--------------------+\n",
      "|account-id|amount|customer-id|           datetime|is_fraud|transaction-id|  type|          @timestamp|\n",
      "+----------+------+-----------+-------------------+--------+--------------+------+--------------------+\n",
      "|         4|  33.0|          2|2023-05-25 00:38:51|   valid|           149|CREDIT|2023-05-25T00:38:...|\n",
      "+----------+------+-----------+-------------------+--------+--------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the write_to_es function on each micro-batch of data\n",
    "value_df_account = df_account_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_account = value_df_account.writeStream.foreachBatch(write_to_es_account).start()\n",
    "\n",
    "value_df_transaction = df_transaction_stream.selectExpr(\"CAST(value AS STRING)\")\n",
    "query_transaction = value_df_transaction.writeStream.foreachBatch(write_to_es_transaction).start()\n",
    "\n",
    "# Wait for the stream to finish\n",
    "#query_account.awaitTermination()\n",
    "query_transaction.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
